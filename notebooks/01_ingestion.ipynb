{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd70958",
   "metadata": {},
   "source": [
    "# Notebook 01 — TMDB Ingestion (RAW Layer)\n",
    "---\n",
    "## Goal\n",
    "Build a single point-in-time snapshot of streaming provider catalog availability across 7 markets and 5 providers, for both movies and TV series.\n",
    "Availability is inferred via the TMDB Discover endpoint filtered by `watch_region`, `with_watch_providers`, and `with_watch_monetization_types=flatrate`.\n",
    "\n",
    "## Outputs\n",
    "- `data/raw/providers_lookup.parquet`\n",
    "- `data/raw/discover_snapshot.parquet`\n",
    "- `data/raw/titles_metadata_raw.parquet`\n",
    "- `data/raw/manifest.json`\n",
    "\n",
    "## Design Notes\n",
    "- Synchronous HTTP requests with exponential backoff on 429 / 5xx\n",
    "- Discover fetch supports checkpoint/resume — safe to interrupt and restart\n",
    "- Metadata fetch supports checkpoint/resume on `(tmdb_id, media_type)` key\n",
    "- TV runtime estimated via median of `episode_run_time` when available\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf662f",
   "metadata": {},
   "source": [
    "## Imports & Configuration\n",
    "Edit `MAX_PAGES_PER_QUERY` and `MAX_TITLES_METADATA` to limit the run for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd224bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SNAPSHOT_DATE = \"2026-02-21\"\n",
    "\n",
    "COUNTRIES = [\"US\", \"GB\", \"IT\", \"DE\", \"FR\", \"ES\", \"KR\"]  # TMDB uses GB for the UK\n",
    "MEDIA_TYPES = [\"movie\", \"tv\"]\n",
    "MONETIZATION_TYPE = \"flatrate\"\n",
    "\n",
    "TARGET_PROVIDER_NAMES = [\n",
    "    \"Netflix\",\n",
    "    \"Amazon Prime Video\",\n",
    "    \"Disney Plus\",\n",
    "    \"Apple TV Plus\",\n",
    "    \"Paramount Plus\",\n",
    "]\n",
    "\n",
    "# Debug knobs — set to None to run full pagination / full metadata pull\n",
    "MAX_PAGES_PER_QUERY = None\n",
    "MAX_TITLES_METADATA = None\n",
    "\n",
    "# Project root detection — works whether running from /notebooks or project root\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == \"notebooks\":\n",
    "    PROJECT_ROOT = cwd.parent\n",
    "else:\n",
    "    PROJECT_ROOT = cwd if (cwd / \"notebooks\").exists() else cwd\n",
    "\n",
    "DATA_RAW_DIR = str(PROJECT_ROOT / \"data\" / \"raw\")\n",
    "os.makedirs(DATA_RAW_DIR, exist_ok=True)\n",
    "\n",
    "DISCOVER_PATH = os.path.join(DATA_RAW_DIR, \"discover_snapshot.parquet\")\n",
    "\n",
    "TMDB_API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not TMDB_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing TMDB_API_KEY environment variable. \"\n",
    "        \"Set it before running and restart your IDE/kernel.\"\n",
    "    )\n",
    "\n",
    "TMDB_BASE_URL = \"https://api.themoviedb.org/3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08b9da",
   "metadata": {},
   "source": [
    "## TMDB Client\n",
    "Synchronous HTTP client with API key injection, configurable timeout, and exponential backoff on 429 and 5xx errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05f745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TMDBClient:\n",
    "    api_key: str\n",
    "    base_url: str = TMDB_BASE_URL\n",
    "    timeout_s: int = 30\n",
    "    sleep_between_calls_s: float = 0.1\n",
    "    max_retries: int = 5\n",
    "\n",
    "    def get(self, path: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params = {**params, \"api_key\": self.api_key}\n",
    "\n",
    "        url = f\"{self.base_url}{path}\"\n",
    "        last_exc: Optional[Exception] = None\n",
    "\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                r = requests.get(url, params=params, timeout=self.timeout_s)\n",
    "                time.sleep(self.sleep_between_calls_s)\n",
    "\n",
    "                if r.status_code == 200:\n",
    "                    return r.json()\n",
    "\n",
    "                if r.status_code in (429, 500, 502, 503, 504):\n",
    "                    wait = (2.0 * attempt) if r.status_code == 429 else (1.5 * attempt)\n",
    "                    time.sleep(wait)\n",
    "                    continue\n",
    "\n",
    "                raise RuntimeError(f\"TMDB error {r.status_code}: {r.text[:300]}\")\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                time.sleep(1.0 * attempt)\n",
    "\n",
    "        raise RuntimeError(f\"TMDB request failed after retries: {last_exc}\")\n",
    "\n",
    "\n",
    "client = TMDBClient(api_key=TMDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b91e77",
   "metadata": {},
   "source": [
    "## Provider Lookup\n",
    "Fetches the full TMDB provider list for movie and TV, then matches target brands using a normalized alias map.\n",
    "Paramount Plus is intentionally kept as two provider IDs (Essential + Premium) to avoid missing titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff61bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected core providers:\n",
      "canonical_provider  provider_id            provider_name\n",
      "Amazon Prime Video            9       Amazon Prime Video\n",
      "     Apple TV Plus          350                 Apple TV\n",
      "       Disney Plus          337              Disney Plus\n",
      "           Netflix            8                  Netflix\n",
      "    Paramount Plus         2616 Paramount Plus Essential\n",
      "    Paramount Plus         2303   Paramount Plus Premium\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def fetch_providers(media_type: str) -> pd.DataFrame:\n",
    "    data = client.get(f\"/watch/providers/{media_type}\", params={\"watch_region\": \"US\"})\n",
    "    results = data.get(\"results\", [])\n",
    "    df = pd.DataFrame(results)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    keep = [c for c in [\"provider_id\", \"provider_name\", \"display_priority\", \"logo_path\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    df[\"media_type_source\"] = media_type\n",
    "    return df\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = s.replace(\"+\", \" plus \")\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "providers_movie = fetch_providers(\"movie\")\n",
    "providers_tv = fetch_providers(\"tv\")\n",
    "providers_all = pd.concat([providers_movie, providers_tv], ignore_index=True).drop_duplicates(\n",
    "    subset=[\"provider_id\", \"provider_name\"]\n",
    ")\n",
    "\n",
    "if providers_all.empty:\n",
    "    raise RuntimeError(\"TMDB provider list is empty. Check API key / connectivity.\")\n",
    "\n",
    "providers_all[\"provider_name_norm\"] = providers_all[\"provider_name\"].map(norm_name)\n",
    "\n",
    "# Exclude add-on channels, plan variants, and third-party storefronts\n",
    "BLACKLIST_TERMS = [\n",
    "    \"with ads\", \"standard with ads\", \"kids\",\n",
    "    \"channel\", \"amazon channel\", \"roku\", \"apple tv channel\",\n",
    "    \"store\", \"free with ads\",\n",
    "]\n",
    "for term in BLACKLIST_TERMS:\n",
    "    providers_all = providers_all[~providers_all[\"provider_name_norm\"].str.contains(norm_name(term), na=False)]\n",
    "\n",
    "# Alias map: canonical brand name → TMDB provider name variants\n",
    "TARGET_PROVIDER_ALIASES = {\n",
    "    \"Netflix\": [\"netflix\"],\n",
    "    \"Amazon Prime Video\": [\"amazon prime video\"],\n",
    "    \"Disney Plus\": [\"disney plus\"],\n",
    "    \"Apple TV Plus\": [\"apple tv\"],  # TMDB uses \"Apple TV\" for the Apple TV+ service\n",
    "    \"Paramount Plus\": [\"paramount plus essential\", \"paramount plus premium\"],\n",
    "}\n",
    "\n",
    "matched_rows = []\n",
    "missing_targets = []\n",
    "\n",
    "for canonical_name, aliases in TARGET_PROVIDER_ALIASES.items():\n",
    "    alias_norm = [norm_name(a) for a in aliases]\n",
    "    hits = providers_all[providers_all[\"provider_name_norm\"].isin(alias_norm)].copy()\n",
    "\n",
    "    if hits.empty:\n",
    "        missing_targets.append(canonical_name)\n",
    "        continue\n",
    "\n",
    "    if canonical_name == \"Paramount Plus\":\n",
    "        for _, h in hits.iterrows():\n",
    "            row = h.copy()\n",
    "            row[\"canonical_provider\"] = canonical_name\n",
    "            matched_rows.append(row)\n",
    "        continue\n",
    "\n",
    "    if \"display_priority\" in hits.columns:\n",
    "        hits = hits.sort_values(\"display_priority\", ascending=True)\n",
    "    best = hits.iloc[0].copy()\n",
    "    best[\"canonical_provider\"] = canonical_name\n",
    "    matched_rows.append(best)\n",
    "\n",
    "if missing_targets:\n",
    "    print(\"Missing target providers:\", missing_targets)\n",
    "    for mt in missing_targets:\n",
    "        key = norm_name(mt).split()[0]\n",
    "        candidates = providers_all[providers_all[\"provider_name_norm\"].str.contains(key, na=False)].head(15)\n",
    "        print(f\"\\nCandidates for '{mt}' (top 15 containing '{key}'):\")\n",
    "        print(candidates[[\"provider_id\", \"provider_name\"]].to_string(index=False))\n",
    "    raise RuntimeError(\"Some target providers not found. Update TARGET_PROVIDER_ALIASES.\")\n",
    "\n",
    "providers_target = pd.DataFrame(matched_rows)\n",
    "providers_map = (\n",
    "    providers_target[[\"provider_id\", \"provider_name\", \"canonical_provider\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"canonical_provider\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "providers_lookup_path = os.path.join(DATA_RAW_DIR, \"providers_lookup.parquet\")\n",
    "providers_map.to_parquet(providers_lookup_path, index=False)\n",
    "\n",
    "print(\"\\nSelected core providers:\")\n",
    "print(providers_map[[\"canonical_provider\", \"provider_id\", \"provider_name\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9f1d5",
   "metadata": {},
   "source": [
    "## Provider Name Debug\n",
    "Quick lookup to verify TMDB provider names for key brands. Useful when updating `TARGET_PROVIDER_ALIASES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fc152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- provider_name contains 'apple' (up to 20) ---\n",
      " provider_id     provider_name\n",
      "         350          Apple TV\n",
      "        2034 Acorn TV Apple TV\n",
      "\n",
      "--- provider_name contains 'paramount' (up to 20) ---\n",
      " provider_id            provider_name\n",
      "        2616 Paramount Plus Essential\n",
      "        2303   Paramount Plus Premium\n",
      "\n",
      "--- provider_name contains 'disney' (up to 20) ---\n",
      " provider_id provider_name\n",
      "         337   Disney Plus\n",
      "         508     DisneyNOW\n",
      "\n",
      "--- provider_name contains 'prime' (up to 20) ---\n",
      " provider_id      provider_name\n",
      "           9 Amazon Prime Video\n",
      "\n",
      "--- provider_name contains 'netflix' (up to 20) ---\n",
      " provider_id provider_name\n",
      "           8       Netflix\n"
     ]
    }
   ],
   "source": [
    "for term in [\"apple\", \"paramount\", \"disney\", \"prime\", \"netflix\"]:\n",
    "    subset = providers_all[providers_all[\"provider_name_norm\"].str.contains(term, na=False)]\n",
    "    print(f\"\\n--- provider_name contains '{term}' (up to 20) ---\")\n",
    "    print(subset[[\"provider_id\", \"provider_name\"]].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cf287",
   "metadata": {},
   "source": [
    "## Discover Availability Snapshot\n",
    "Queries `/discover/movie` and `/discover/tv` for each `(provider, country)` combination filtered to flatrate monetization.\n",
    "Supports checkpoint/resume — if `discover_snapshot.parquet` already exists the fetch is skipped entirely.\n",
    "\n",
    "> Runtime: up to ~80 minutes for a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab029581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing discover snapshot: 127399 rows — skipping fetch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmdb_id</th>\n",
       "      <th>media_type</th>\n",
       "      <th>provider_id</th>\n",
       "      <th>country</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1168190</td>\n",
       "      <td>movie</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>2026-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1317672</td>\n",
       "      <td>movie</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>2026-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755898</td>\n",
       "      <td>movie</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>2026-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1503900</td>\n",
       "      <td>movie</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>2026-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14874</td>\n",
       "      <td>movie</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>2026-02-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tmdb_id media_type  provider_id country snapshot_date\n",
       "0  1168190      movie            9      US    2026-02-21\n",
       "1  1317672      movie            9      US    2026-02-21\n",
       "2   755898      movie            9      US    2026-02-21\n",
       "3  1503900      movie            9      US    2026-02-21\n",
       "4    14874      movie            9      US    2026-02-21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "BAR_WIDTH = 40\n",
    "\n",
    "if os.path.exists(DISCOVER_PATH):\n",
    "    discover_df = pd.read_parquet(DISCOVER_PATH)\n",
    "    print(f\"Loaded existing discover snapshot: {len(discover_df)} rows — skipping fetch.\")\n",
    "\n",
    "else:\n",
    "    jobs = []\n",
    "    for media_type in MEDIA_TYPES:\n",
    "        for country in COUNTRIES:\n",
    "            for _, prow in providers_map.iterrows():\n",
    "                jobs.append({\n",
    "                    \"media_type\": media_type,\n",
    "                    \"country\": country,\n",
    "                    \"provider_id\": int(prow[\"provider_id\"]),\n",
    "                    \"provider_name\": str(prow[\"provider_name\"]),\n",
    "                    \"canonical\": str(prow[\"canonical_provider\"]),\n",
    "                })\n",
    "\n",
    "    print(\"Probing to estimate total pages across all jobs...\")\n",
    "    probe_cache = {}\n",
    "    total_pages_all = 0\n",
    "    for j in jobs:\n",
    "        path = f\"/discover/{j['media_type']}\"\n",
    "        params = {\n",
    "            \"watch_region\": j[\"country\"],\n",
    "            \"with_watch_providers\": j[\"provider_id\"],\n",
    "            \"with_watch_monetization_types\": MONETIZATION_TYPE,\n",
    "            \"page\": 1,\n",
    "        }\n",
    "        try:\n",
    "            data = client.get(path, params=params)\n",
    "            total_pages = int(data.get(\"total_pages\", 1) or 1)\n",
    "            eff = min(total_pages, MAX_PAGES_PER_QUERY or 500, 500)\n",
    "            eff = max(1, eff)\n",
    "            probe_cache[(j['media_type'], j['country'], j['provider_id'])] = {\n",
    "                \"first_page_results\": data.get(\"results\", []),\n",
    "                \"effective_total_pages\": eff,\n",
    "            }\n",
    "            total_pages_all += eff\n",
    "        except Exception as e:\n",
    "            print(f\"Probe failed for {j['media_type']}|{j['country']}|{j['provider_id']}: {e}\")\n",
    "            probe_cache[(j['media_type'], j['country'], j['provider_id'])] = {\n",
    "                \"first_page_results\": [],\n",
    "                \"effective_total_pages\": 0,\n",
    "            }\n",
    "\n",
    "    if total_pages_all == 0:\n",
    "        print(\"No pages to fetch. Aborting discover.\")\n",
    "        discover_df = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"Estimated total pages to fetch: {total_pages_all}\")\n",
    "\n",
    "        pages_done = 0\n",
    "        start_time = time.time()\n",
    "        rows_acc: List[Dict[str, Any]] = []\n",
    "\n",
    "        for j in jobs:\n",
    "            media_type = j[\"media_type\"]\n",
    "            country = j[\"country\"]\n",
    "            pid = j[\"provider_id\"]\n",
    "\n",
    "            cached = probe_cache.get((media_type, country, pid), {})\n",
    "            effective_total = cached.get(\"effective_total_pages\", 0)\n",
    "            if effective_total <= 0:\n",
    "                continue\n",
    "\n",
    "            for page in range(1, effective_total + 1):\n",
    "                if page == 1 and cached.get(\"first_page_results\") is not None:\n",
    "                    results = cached.get(\"first_page_results\", [])\n",
    "                else:\n",
    "                    data = client.get(f\"/discover/{media_type}\", params={\n",
    "                        \"watch_region\": country,\n",
    "                        \"with_watch_providers\": pid,\n",
    "                        \"with_watch_monetization_types\": MONETIZATION_TYPE,\n",
    "                        \"page\": page,\n",
    "                    })\n",
    "                    results = data.get(\"results\", [])\n",
    "\n",
    "                for r in results:\n",
    "                    tid = r.get(\"id\")\n",
    "                    if tid is None:\n",
    "                        continue\n",
    "                    rows_acc.append({\n",
    "                        \"tmdb_id\": int(tid),\n",
    "                        \"media_type\": media_type,\n",
    "                        \"provider_id\": pid,\n",
    "                        \"country\": country,\n",
    "                        \"snapshot_date\": SNAPSHOT_DATE,\n",
    "                    })\n",
    "\n",
    "                pages_done += 1\n",
    "                proportion = min(1.0, pages_done / float(total_pages_all))\n",
    "                filled = int(proportion * BAR_WIDTH)\n",
    "                bar = \"=\" * filled + \"-\" * (BAR_WIDTH - filled)\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time_per_page = (elapsed / pages_done) if pages_done > 0 else 0.0\n",
    "                eta_seconds = max(0, total_pages_all - pages_done) * avg_time_per_page\n",
    "                print(\n",
    "                    f\"[DISCOVER] [{bar}] {pages_done}/{total_pages_all} pages — elapsed {int(elapsed)}s ETA {eta_seconds/60:.1f}m\",\n",
    "                    end='\\r', flush=True,\n",
    "                )\n",
    "\n",
    "        print()\n",
    "        discover_df = pd.DataFrame(rows_acc)\n",
    "\n",
    "        if not discover_df.empty:\n",
    "            discover_df = discover_df.drop_duplicates(\n",
    "                subset=[\"tmdb_id\", \"media_type\", \"provider_id\", \"country\"]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "        discover_df.to_parquet(DISCOVER_PATH, index=False)\n",
    "        total_elapsed = time.time() - start_time\n",
    "        print(f\"Discover complete. Rows: {len(discover_df)} | Elapsed: {total_elapsed/60:.1f} min\")\n",
    "\n",
    "availability_df = discover_df\n",
    "availability_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae396a",
   "metadata": {},
   "source": [
    "## Unique Titles List\n",
    "Deduplicate discover results to a unique `(tmdb_id, media_type)` list. Metadata is pulled once per title regardless of how many providers or markets carry it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe79f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles total: 46973\n",
      "Unique titles to pull metadata for: 46973\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmdb_id</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tmdb_id media_type\n",
       "0        5      movie\n",
       "1        6      movie\n",
       "2       11      movie\n",
       "3       12      movie\n",
       "4       13      movie"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if discover_df.empty:\n",
    "    raise RuntimeError(\"discover_df is empty. Check TMDB discover filters and provider IDs.\")\n",
    "\n",
    "unique_titles = (\n",
    "    discover_df[[\"tmdb_id\", \"media_type\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"media_type\", \"tmdb_id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "unique_titles_for_pull = unique_titles.head(MAX_TITLES_METADATA) if MAX_TITLES_METADATA else unique_titles\n",
    "\n",
    "print(\"Unique titles total:\", len(unique_titles))\n",
    "print(\"Unique titles to pull metadata for:\", len(unique_titles_for_pull))\n",
    "unique_titles_for_pull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c18c4",
   "metadata": {},
   "source": [
    "## Metadata Helper Functions\n",
    "Normalizes movie and TV detail responses into a unified flat schema. TV runtime is estimated via median of `episode_run_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a155ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_year(date_str: Optional[str]) -> Optional[int]:\n",
    "    if not date_str or not isinstance(date_str, str) or len(date_str) < 4:\n",
    "        return None\n",
    "    try:\n",
    "        return int(date_str[:4])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def median_or_none(values: Any) -> Optional[int]:\n",
    "    if values is None:\n",
    "        return None\n",
    "    if isinstance(values, list) and len(values) > 0:\n",
    "        nums = [v for v in values if isinstance(v, (int, float)) and v is not None]\n",
    "        if not nums:\n",
    "            return None\n",
    "        return int(pd.Series(nums).median())\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_origin_country(data: Dict[str, Any], media_type: str) -> Optional[str]:\n",
    "    oc = data.get(\"origin_country\")\n",
    "    if isinstance(oc, list) and len(oc) > 0:\n",
    "        return oc[0]\n",
    "    if isinstance(oc, str) and oc:\n",
    "        return oc\n",
    "    prod = data.get(\"production_countries\")\n",
    "    if isinstance(prod, list) and len(prod) > 0 and isinstance(prod[0], dict):\n",
    "        return prod[0].get(\"iso_3166_1\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_genres(data: Dict[str, Any]) -> Tuple[Optional[str], str]:\n",
    "    genres = data.get(\"genres\") or []\n",
    "    if not isinstance(genres, list):\n",
    "        return None, \"\"\n",
    "    names = [g.get(\"name\", \"\") for g in genres if isinstance(g, dict) and g.get(\"name\")]\n",
    "    primary = names[0] if names else None\n",
    "    return primary, \"|\".join(names)\n",
    "\n",
    "\n",
    "def fetch_title_details(tmdb_id: int, media_type: str) -> Dict[str, Any]:\n",
    "    data = client.get(f\"/{media_type}/{tmdb_id}\", params={})\n",
    "\n",
    "    if media_type == \"movie\":\n",
    "        title = data.get(\"title\")\n",
    "        date_field = data.get(\"release_date\")\n",
    "        runtime_min = data.get(\"runtime\")\n",
    "    else:\n",
    "        title = data.get(\"name\")\n",
    "        date_field = data.get(\"first_air_date\")\n",
    "        runtime_min = median_or_none(data.get(\"episode_run_time\"))\n",
    "\n",
    "    primary_genre, genres_str = normalize_genres(data)\n",
    "    origin_country = normalize_origin_country(data, media_type)\n",
    "\n",
    "    return {\n",
    "        \"tmdb_id\": tmdb_id,\n",
    "        \"media_type\": media_type,\n",
    "        \"title\": title,\n",
    "        \"release_date\": date_field,\n",
    "        \"release_year\": safe_year(date_field),\n",
    "        \"primary_genre\": primary_genre,\n",
    "        \"genres\": genres_str,\n",
    "        \"origin_country\": origin_country,\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"runtime_min\": runtime_min,\n",
    "        \"snapshot_date\": SNAPSHOT_DATE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357d718",
   "metadata": {},
   "source": [
    "## Title Metadata Pull (Checkpoint + Resume)\n",
    "Fetches detailed metadata for each unique `(tmdb_id, media_type)`. Saves a checkpoint every `SAVE_EVERY` titles — safe to interrupt and restart.\n",
    "\n",
    "> Runtime: up to ~6 hours for 46k titles at 0.1s per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1337e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint. Already have 46973 (tmdb_id, media_type) entries.\n",
      "Titles to process now: 0\n",
      "\n",
      "Metadata pull complete.\n",
      "Total titles now stored: 46973\n",
      "Total elapsed time: 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(DATA_RAW_DIR, \"titles_metadata_raw.parquet\")\n",
    "SAVE_EVERY = 500\n",
    "BAR_WIDTH = 40\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    titles_meta_df = pd.read_parquet(CHECKPOINT_PATH)\n",
    "    done = set(zip(titles_meta_df[\"tmdb_id\"].astype(int), titles_meta_df[\"media_type\"].astype(str)))\n",
    "    print(f\"Resuming from checkpoint. Already have {len(done)} (tmdb_id, media_type) entries.\")\n",
    "else:\n",
    "    titles_meta_df = pd.DataFrame()\n",
    "    done = set()\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "unique_titles = discover_df[[\"tmdb_id\", \"media_type\"]].drop_duplicates().reset_index(drop=True)\n",
    "unique_titles[\"_key\"] = list(zip(unique_titles[\"tmdb_id\"].astype(int), unique_titles[\"media_type\"].astype(str)))\n",
    "unique_titles = unique_titles[~unique_titles[\"_key\"].isin(done)].drop(columns=\"_key\").reset_index(drop=True)\n",
    "\n",
    "total_to_process = len(unique_titles)\n",
    "print(f\"Titles to process now: {total_to_process}\")\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "new_rows = []\n",
    "\n",
    "for idx, row in unique_titles.iterrows():\n",
    "    tmdb_id = int(row[\"tmdb_id\"])\n",
    "    media_type = row[\"media_type\"]\n",
    "\n",
    "    try:\n",
    "        meta = fetch_title_details(tmdb_id, media_type)\n",
    "        new_rows.append(meta)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR on {tmdb_id} ({media_type}): {e}\")\n",
    "        continue\n",
    "\n",
    "    processed_count += 1\n",
    "\n",
    "    proportion = min(1.0, processed_count / float(total_to_process))\n",
    "    filled = int(proportion * BAR_WIDTH)\n",
    "    bar = \"=\" * filled + \"-\" * (BAR_WIDTH - filled)\n",
    "    elapsed = time.time() - start_time\n",
    "    avg = elapsed / processed_count if processed_count > 0 else 0\n",
    "    eta = max(0, total_to_process - processed_count) * avg\n",
    "    eta_h = int(eta // 3600)\n",
    "    eta_m = int((eta % 3600) // 60)\n",
    "    eta_s = int(eta % 60)\n",
    "    eta_str = f\"{eta_h}h {eta_m:02d}m {eta_s:02d}s\" if eta_h > 0 else f\"{eta_m}m {eta_s:02d}s\"\n",
    "\n",
    "    print(\n",
    "        f\"[METADATA] [{bar}] {processed_count}/{total_to_process} — elapsed {int(elapsed)}s ETA {eta_str}\",\n",
    "        end='\\r', flush=True,\n",
    "    )\n",
    "\n",
    "    if processed_count % SAVE_EVERY == 0:\n",
    "        temp_df = pd.DataFrame(new_rows)\n",
    "        titles_meta_df = pd.concat([titles_meta_df, temp_df], ignore_index=True)\n",
    "        titles_meta_df.to_parquet(CHECKPOINT_PATH, index=False)\n",
    "        new_rows = []\n",
    "        print(f\"\\nCheckpoint saved at {processed_count} new titles.\")\n",
    "\n",
    "if new_rows:\n",
    "    temp_df = pd.DataFrame(new_rows)\n",
    "    titles_meta_df = pd.concat([titles_meta_df, temp_df], ignore_index=True)\n",
    "    titles_meta_df.to_parquet(CHECKPOINT_PATH, index=False)\n",
    "\n",
    "total_elapsed = time.time() - start_time\n",
    "print(f\"\\nMetadata pull complete.\")\n",
    "print(f\"Total titles now stored: {len(titles_meta_df)}\")\n",
    "print(f\"Total elapsed time: {total_elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2058fb4",
   "metadata": {},
   "source": [
    "## RAW Manifest\n",
    "Lightweight versioning file describing what was ingested and with which parameters. Written to `data/raw/manifest.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3f8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest written to: c:\\Users\\matt\\OneDrive\\Desktop\\Data_Projects\\Streaming_Benchmark_Project\\data\\raw\\manifest.json\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "    \"snapshot_date\": SNAPSHOT_DATE,\n",
    "    \"source\": \"TMDB\",\n",
    "    \"countries\": COUNTRIES,\n",
    "    \"media_types\": MEDIA_TYPES,\n",
    "    \"monetization_filter\": MONETIZATION_TYPE,\n",
    "    \"providers\": providers_map.to_dict(orient=\"records\"),\n",
    "    \"debug_limits\": {\n",
    "        \"max_pages_per_query\": MAX_PAGES_PER_QUERY,\n",
    "        \"max_titles_metadata\": MAX_TITLES_METADATA,\n",
    "    },\n",
    "    \"row_counts\": {\n",
    "        \"providers_lookup\": int(len(providers_target)),\n",
    "        \"discover_snapshot\": int(len(discover_df)),\n",
    "        \"titles_metadata_raw\": int(len(titles_meta_df)),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Availability inferred via TMDB Discover endpoints filtered by watch providers.\",\n",
    "        \"Only flatrate (subscription streaming) availability is included.\",\n",
    "        \"TV runtime approximated via median of episode_run_time when available.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "manifest_path = os.path.join(DATA_RAW_DIR, \"manifest.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Manifest written to:\", manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e23199",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "Quick validation to confirm the snapshot is non-empty and missingness is within expected bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11e2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country  provider_id  distinct_titles canonical_provider\n",
      "     DE            8             8693            Netflix\n",
      "     DE            9             8010 Amazon Prime Video\n",
      "     DE          337             3482        Disney Plus\n",
      "     DE          350              309      Apple TV Plus\n",
      "     ES            8             9385            Netflix\n",
      "     ES          337             3519        Disney Plus\n",
      "     ES          350              309      Apple TV Plus\n",
      "     FR            8             8644            Netflix\n",
      "     FR          337             3612        Disney Plus\n",
      "     FR         2303              877     Paramount Plus\n",
      "     FR          350              305      Apple TV Plus\n",
      "     GB            9            13387 Amazon Prime Video\n",
      "     GB            8             8853            Netflix\n",
      "     GB          337             4058        Disney Plus\n",
      "     GB         2303              932     Paramount Plus\n",
      "     GB          350              311      Apple TV Plus\n",
      "     IT            8             9004            Netflix\n",
      "     IT          337             3566        Disney Plus\n",
      "     IT          350              314      Apple TV Plus\n",
      "     KR            8             8480            Netflix\n",
      "runtime_min          0.112597\n",
      "primary_genre        0.032806\n",
      "origin_country       0.009474\n",
      "release_year         0.003768\n",
      "media_type           0.000000\n",
      "tmdb_id              0.000000\n",
      "title                0.000000\n",
      "release_date         0.000000\n",
      "original_language    0.000000\n",
      "genres               0.000000\n",
      "vote_average         0.000000\n",
      "vote_count           0.000000\n",
      "popularity           0.000000\n",
      "snapshot_date        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Distinct titles per provider x country\n",
    "avail_check = (\n",
    "    discover_df.groupby([\"country\", \"provider_id\"])[\"tmdb_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"distinct_titles\")\n",
    "    .merge(providers_map[[\"provider_id\", \"canonical_provider\"]], on=\"provider_id\", how=\"left\")\n",
    "    .sort_values([\"country\", \"distinct_titles\"], ascending=[True, False])\n",
    ")\n",
    "print(avail_check.head(20).to_string(index=False))\n",
    "\n",
    "# Missingness report on raw metadata\n",
    "missing_report = titles_meta_df.isna().mean().sort_values(ascending=False)\n",
    "print(missing_report.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
